{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa51DI0YkSLIzQwMVS5/uA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hailemicael/zewdacadamyy.github.io/blob/master/Copy_of_NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCcc77enUMnF",
        "outputId": "f523ab2a-c5b7-40a6-fcd0-547a1e5ea638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical Text:\n",
            " <link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1066479718\"><p><b>Cardiology</b> (from Ancient Greek <i> </i>καρδίᾱ<i> (kardiā)</i> 'heart', and <i> -</i>λογία<i> (-logia)</i> 'study') is the study of the heart. Cardiology is a branch of medicine that deals with disorders of the heart and the cardiovascular system. The field includes medical diagnosis and treatment of congenital heart defects, coronary artery disease, heart failure, valvular heart disease, and electrophysiology. Physicians who specialize in this field of medicine are called <b>cardiologists</b>, a specialty of internal medicine. Pediatric cardiologists are pediatricians who specialize in cardiology. Physicians who specialize in cardiac surgery are called <b>cardiothoracic surgeons</b> or <b>cardiac surgeons</b>, a specialty of general surgery.</p>\n",
            "\n",
            "Non-Medical Text:\n",
            " <p class=\"mw-empty-elt\">\n",
            "</p>\n",
            "\n",
            "\n",
            "<p>The <b>history of art</b> focuses on objects made by humans for any number of spiritual, narrative, philosophical, symbolic, conceptual, documentary, decorative, and even functional and other purposes, but with a primary emphasis on its aesthetic visual form. Visual art can be classified in diverse ways, such as separating fine arts from applied arts; inclusively focusing on human creativity; or focusing on different media such as architecture, sculpture, painting, film, photography, and graphic arts. In recent years, technological advances have led to video art, computer art, performance art, animation, television, and videogames.\n",
            "</p><p>The history of art is often told as a chronology of masterpieces created during each civilization. It can thus be framed as a story of high culture, epitomized by the Wonders of the World. On the other hand, vernacular art expressions can also be integrated into art historical narratives, referred to as folk arts or craft. The more closely that an art historian engages with these latter forms of low culture, the more likely it is that they will identify their work as examining visual culture or material culture, or as contributing to fields related to art history, such as anthropology or archaeology. In the latter cases, art objects may be referred to as archeological artifacts.\n",
            "</p>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_text(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    text_content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "    return text_content\n",
        "\n",
        "# Get text content for \"Cardiology\" and \"History of Art\"\n",
        "medical_text = get_wikipedia_text(\"Cardiology\")\n",
        "non_medical_text = get_wikipedia_text(\"History_of_art\")\n",
        "\n",
        "print(\"Medical Text:\\n\", medical_text)\n",
        "print(\"\\nNon-Medical Text:\\n\", non_medical_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_wikipedia_text(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    text_content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "    return text_content\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "# Get text content for \"Cardiology\" and \"History of Art\"\n",
        "medical_text = get_wikipedia_text(\"Cardiology\")\n",
        "non_medical_text = get_wikipedia_text(\"History_of_art\")\n",
        "\n",
        "# Remove HTML tags\n",
        "medical_text = remove_html_tags(medical_text)\n",
        "non_medical_text = remove_html_tags(non_medical_text)\n",
        "\n",
        "print(\"Medical Text (without HTML tags):\\n\", medical_text)\n",
        "print(\"\\nNon-Medical Text (without HTML tags):\\n\", non_medical_text)\n",
        "\n",
        "# Create a DataFrame with annotated data\n",
        "data = {\n",
        "    \"text\": [\n",
        "        medical_text,\n",
        "        non_medical_text,\n",
        "    ],\n",
        "    \"label\": [\"medical\", \"non-medical\"],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the annotated data to a CSV file\n",
        "df.to_csv(\"annotated_data.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJY-bKGvVyK7",
        "outputId": "804e6618-6330-4d02-e169-302b1ccc198f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medical Text (without HTML tags):\n",
            " Cardiology (from Ancient Greek  καρδίᾱ (kardiā) 'heart', and  -λογία (-logia) 'study') is the study of the heart. Cardiology is a branch of medicine that deals with disorders of the heart and the cardiovascular system. The field includes medical diagnosis and treatment of congenital heart defects, coronary artery disease, heart failure, valvular heart disease, and electrophysiology. Physicians who specialize in this field of medicine are called cardiologists, a specialty of internal medicine. Pediatric cardiologists are pediatricians who specialize in cardiology. Physicians who specialize in cardiac surgery are called cardiothoracic surgeons or cardiac surgeons, a specialty of general surgery.\n",
            "\n",
            "Non-Medical Text (without HTML tags):\n",
            " \n",
            "\n",
            "The history of art focuses on objects made by humans for any number of spiritual, narrative, philosophical, symbolic, conceptual, documentary, decorative, and even functional and other purposes, but with a primary emphasis on its aesthetic visual form. Visual art can be classified in diverse ways, such as separating fine arts from applied arts; inclusively focusing on human creativity; or focusing on different media such as architecture, sculpture, painting, film, photography, and graphic arts. In recent years, technological advances have led to video art, computer art, performance art, animation, television, and videogames.\n",
            "The history of art is often told as a chronology of masterpieces created during each civilization. It can thus be framed as a story of high culture, epitomized by the Wonders of the World. On the other hand, vernacular art expressions can also be integrated into art historical narratives, referred to as folk arts or craft. The more closely that an art historian engages with these latter forms of low culture, the more likely it is that they will identify their work as examining visual culture or material culture, or as contributing to fields related to art history, such as anthropology or archaeology. In the latter cases, art objects may be referred to as archeological artifacts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the annotated data\n",
        "df = pd.read_csv(\"/content/annotated_data.csv\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df[\"text\"], df[\"label\"], test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train, train_labels)\n",
        "\n",
        "# Make predictions on the test set\n",
        "naive_bayes_predictions = naive_bayes_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Naive Bayes model\n",
        "naive_bayes_accuracy = accuracy_score(test_labels, naive_bayes_predictions)\n",
        "naive_bayes_report = classification_report(test_labels, naive_bayes_predictions)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\", naive_bayes_accuracy)\n",
        "print(\"\\nNaive Bayes Classification Report:\\n\", naive_bayes_report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41hsgSA_W0FC",
        "outputId": "5064e658-5db5-401b-8535-79aafcd333ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.0\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.00      0.00      0.00       0.0\n",
            " non-medical       0.00      0.00      0.00       1.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets with a smaller test_size\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ilSMvHvBY9hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining set labels distribution:\")\n",
        "print(train_labels.value_counts())\n",
        "\n",
        "print(\"\\nTesting set labels distribution:\")\n",
        "print(test_labels.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJzFF2htZCF5",
        "outputId": "fa2f467d-3c3b-401d-fca8-ffe44d6159ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set labels distribution:\n",
            "medical    1\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Testing set labels distribution:\n",
            "non-medical    1\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "    return content\n",
        "\n",
        "# List of medical and non-medical topics to fetch\n",
        "medical_topics = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\"]\n",
        "non_medical_topics = [\"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\"]\n",
        "\n",
        "# Fetch content for medical topics\n",
        "medical_content = [fetch_wikipedia_content(topic) for topic in medical_topics]\n",
        "\n",
        "# Fetch content for non-medical topics\n",
        "non_medical_content = [fetch_wikipedia_content(topic) for topic in non_medical_topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content, \"label\": [\"medical\"] * len(medical_content)}\n",
        "non_medical_data = {\"text\": non_medical_content, \"label\": [\"non-medical\"] * len(non_medical_content)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat([df, df_medical, df_non_medical], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7_1CwplZ8aP",
        "outputId": "d83d1877-de74-4a22-b224-ca85ae0c3c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Cardiology (from Ancient Greek  καρδίᾱ (kardiā...      medical\n",
            "1   \\n\\nThe history of art focuses on objects made...  non-medical\n",
            "2   <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "3   <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "4   <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "5   <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "6   <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "7   <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "8   <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "9   <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "10  <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "11  <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "12  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "13  <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "14  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "15  <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "16  <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "17  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    clean_html = re.sub(r\"<.*?>\", \"\", text)\n",
        "\n",
        "    # Remove references (e.g., [1], [2])\n",
        "    clean_references = re.sub(r\"\\[\\d+\\]\", \"\", clean_html)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_references).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# List of medical and non-medical topics to fetch\n",
        "medical_topics = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\"]\n",
        "non_medical_topics = [\"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\"]\n",
        "\n",
        "# Fetch content for medical topics\n",
        "medical_content = [fetch_wikipedia_content(topic) for topic in medical_topics]\n",
        "\n",
        "# Fetch content for non-medical topics\n",
        "non_medical_content = [fetch_wikipedia_content(topic) for topic in non_medical_topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content, \"label\": [\"medical\"] * len(medical_content)}\n",
        "non_medical_data = {\"text\": non_medical_content, \"label\": [\"non-medical\"] * len(non_medical_content)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat([df, df_medical, df_non_medical], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCrp0U1UaZ1C",
        "outputId": "069a9373-3554-4250-fb3d-780c713bd0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Cardiology (from Ancient Greek  καρδίᾱ (kardiā...      medical\n",
            "1   \\n\\nThe history of art focuses on objects made...  non-medical\n",
            "2   <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "3   <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "4   <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "5   <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "6   <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "7   <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "8   <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "9   <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "10  <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "11  <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "12  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "13  <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "14  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "15  <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "16  <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "17  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "18  Medicine is the science and practice of caring...      medical\n",
            "19  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "20  Surgery is a medical specialty that uses manua...      medical\n",
            "21  In common usage and medicine, health, accordin...      medical\n",
            "22  Art is a diverse range of human activity, and ...  non-medical\n",
            "23  <!-- NewPP limit report Parsed by mw1449 Cache...  non-medical\n",
            "24  Literature is any collection of written work, ...  non-medical\n",
            "25  Philosophy (love of wisdom in ancient Greek) i...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove references (e.g., [1], [2])\n",
        "    clean_text = re.sub(r\"\\[\\d+\\]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# List of medical and non-medical topics to fetch\n",
        "medical_topics = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\"]\n",
        "non_medical_topics = [\"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\"]\n",
        "\n",
        "# Fetch content for medical topics\n",
        "medical_content = [fetch_wikipedia_content(topic) for topic in medical_topics]\n",
        "\n",
        "# Fetch content for non-medical topics\n",
        "non_medical_content = [fetch_wikipedia_content(topic) for topic in non_medical_topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content, \"label\": [\"medical\"] * len(medical_content)}\n",
        "non_medical_data = {\"text\": non_medical_content, \"label\": [\"non-medical\"] * len(non_medical_content)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat([df, df_medical, df_non_medical], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y71Lws8aqmz",
        "outputId": "f0f4129d-da2b-4dff-ba06-37da06cc77da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Cardiology (from Ancient Greek  καρδίᾱ (kardiā...      medical\n",
            "1   \\n\\nThe history of art focuses on objects made...  non-medical\n",
            "2   <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "3   <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "4   <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "5   <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "6   <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "7   <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "8   <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "9   <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "10  <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "11  <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "12  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "13  <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "14  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "15  <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "16  <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "17  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "18  Medicine is the science and practice of caring...      medical\n",
            "19  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "20  Surgery is a medical specialty that uses manua...      medical\n",
            "21  In common usage and medicine, health, accordin...      medical\n",
            "22  Art is a diverse range of human activity, and ...  non-medical\n",
            "23  <!-- NewPP limit report Parsed by mw1449 Cache...  non-medical\n",
            "24  Literature is any collection of written work, ...  non-medical\n",
            "25  Philosophy (love of wisdom in ancient Greek) i...  non-medical\n",
            "26  Medicine is the science and practice of caring...      medical\n",
            "27  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "28  Surgery is a medical specialty that uses manua...      medical\n",
            "29  In common usage and medicine, health , accordi...      medical\n",
            "30  Art is a diverse range of human activity, and ...  non-medical\n",
            "31                                                     non-medical\n",
            "32  Literature is any collection of written work, ...  non-medical\n",
            "33  Philosophy ( love of wisdom in ancient Greek) ...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# List of medical and non-medical topics to fetch\n",
        "medical_topics = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\"]\n",
        "non_medical_topics = [\"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\"]\n",
        "\n",
        "# Fetch content for medical topics\n",
        "medical_content = [fetch_wikipedia_content(topic) for topic in medical_topics]\n",
        "\n",
        "# Fetch content for non-medical topics\n",
        "non_medical_content = [fetch_wikipedia_content(topic) for topic in non_medical_topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content, \"label\": [\"medical\"] * len(medical_content)}\n",
        "non_medical_data = {\"text\": non_medical_content, \"label\": [\"non-medical\"] * len(non_medical_content)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat([df, df_medical, df_non_medical], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUW0Rkaoa7sw",
        "outputId": "e53b9f7a-5051-4082-ca76-c9b7c6c6f867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Cardiology (from Ancient Greek  καρδίᾱ (kardiā...      medical\n",
            "1   \\n\\nThe history of art focuses on objects made...  non-medical\n",
            "2   <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "3   <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "4   <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "5   <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "6   <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "7   <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "8   <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "9   <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "10  <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "11  <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "12  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "13  <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "14  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "15  <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "16  <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "17  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "18  Medicine is the science and practice of caring...      medical\n",
            "19  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "20  Surgery is a medical specialty that uses manua...      medical\n",
            "21  In common usage and medicine, health, accordin...      medical\n",
            "22  Art is a diverse range of human activity, and ...  non-medical\n",
            "23  <!-- NewPP limit report Parsed by mw1449 Cache...  non-medical\n",
            "24  Literature is any collection of written work, ...  non-medical\n",
            "25  Philosophy (love of wisdom in ancient Greek) i...  non-medical\n",
            "26  Medicine is the science and practice of caring...      medical\n",
            "27  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "28  Surgery is a medical specialty that uses manua...      medical\n",
            "29  In common usage and medicine, health , accordi...      medical\n",
            "30  Art is a diverse range of human activity, and ...  non-medical\n",
            "31                                                     non-medical\n",
            "32  Literature is any collection of written work, ...  non-medical\n",
            "33  Philosophy ( love of wisdom in ancient Greek) ...  non-medical\n",
            "34  Medicine is the science and practice of caring...      medical\n",
            "35  Cardiology from Ancient Greek kardi heart and ...      medical\n",
            "36  Surgery is a medical specialty that uses manua...      medical\n",
            "37  In common usage and medicine health according ...      medical\n",
            "38  Art is a diverse range of human activity and i...  non-medical\n",
            "39                                                     non-medical\n",
            "40  Literature is any collection of written work b...  non-medical\n",
            "41  Philosophy love of wisdom in ancient Greek is ...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# List of medical and non-medical topics to fetch\n",
        "medical_topics = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\"]\n",
        "non_medical_topics = [\"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\"]\n",
        "\n",
        "# Fetch content for medical topics\n",
        "medical_content = [fetch_wikipedia_content(topic) for topic in medical_topics]\n",
        "\n",
        "# Fetch content for non-medical topics\n",
        "non_medical_content = [fetch_wikipedia_content(topic) for topic in non_medical_topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content, \"label\": [\"medical\"] * len(medical_content)}\n",
        "non_medical_data = {\"text\": non_medical_content, \"label\": [\"non-medical\"] * len(non_medical_content)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat([df, df_medical, df_non_medical], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsB8yp-JbHX-",
        "outputId": "eefd1b55-a8b3-4b2e-82cc-67e451a9d61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Cardiology (from Ancient Greek  καρδίᾱ (kardiā...      medical\n",
            "1   \\n\\nThe history of art focuses on objects made...  non-medical\n",
            "2   <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "3   <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "4   <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "5   <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "6   <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "7   <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "8   <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "9   <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "10  <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Medicine...      medical\n",
            "11  <link rel=\"mw-deduplicated-inline-style\" href=...      medical\n",
            "12  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Surger...      medical\n",
            "13  <p class=\"mw-empty-elt\">\\n</p><p>In common usa...      medical\n",
            "14  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Art</b...  non-medical\n",
            "15  <!-- \\nNewPP limit report\\nParsed by mw1449\\nC...  non-medical\n",
            "16  <p class=\"mw-empty-elt\">\\n</p>\\n<link rel=\"mw-...  non-medical\n",
            "17  <p class=\"mw-empty-elt\">\\n\\n\\n</p>\\n<link rel=...  non-medical\n",
            "18  Medicine is the science and practice of caring...      medical\n",
            "19  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "20  Surgery is a medical specialty that uses manua...      medical\n",
            "21  In common usage and medicine, health, accordin...      medical\n",
            "22  Art is a diverse range of human activity, and ...  non-medical\n",
            "23  <!-- NewPP limit report Parsed by mw1449 Cache...  non-medical\n",
            "24  Literature is any collection of written work, ...  non-medical\n",
            "25  Philosophy (love of wisdom in ancient Greek) i...  non-medical\n",
            "26  Medicine is the science and practice of caring...      medical\n",
            "27  Cardiology (from Ancient Greek καρδίᾱ (kardiā)...      medical\n",
            "28  Surgery is a medical specialty that uses manua...      medical\n",
            "29  In common usage and medicine, health , accordi...      medical\n",
            "30  Art is a diverse range of human activity, and ...  non-medical\n",
            "31                                                     non-medical\n",
            "32  Literature is any collection of written work, ...  non-medical\n",
            "33  Philosophy ( love of wisdom in ancient Greek) ...  non-medical\n",
            "34  Medicine is the science and practice of caring...      medical\n",
            "35  Cardiology from Ancient Greek kardi heart and ...      medical\n",
            "36  Surgery is a medical specialty that uses manua...      medical\n",
            "37  In common usage and medicine health according ...      medical\n",
            "38  Art is a diverse range of human activity and i...  non-medical\n",
            "39                                                     non-medical\n",
            "40  Literature is any collection of written work b...  non-medical\n",
            "41  Philosophy love of wisdom in ancient Greek is ...  non-medical\n",
            "42  Medicine is the science and practice of caring...      medical\n",
            "43  Cardiology from Ancient Greek kardi heart and ...      medical\n",
            "44  Surgery is a medical specialty that uses manua...      medical\n",
            "45  In common usage and medicine health according ...      medical\n",
            "46  Art is a diverse range of human activity and i...  non-medical\n",
            "47                                                     non-medical\n",
            "48  Literature is any collection of written work b...  non-medical\n",
            "49  Philosophy love of wisdom in ancient Greek is ...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# List of medical and non-medical topics to fetch\n",
        "medical_topics = [\"Medicine\", \"Cardiology\", \"Surgery\", \"Health\"]\n",
        "non_medical_topics = [\"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\"]\n",
        "\n",
        "# Fetch content for medical topics\n",
        "medical_content = [fetch_wikipedia_content(topic) for topic in medical_topics]\n",
        "\n",
        "# Fetch content for non-medical topics\n",
        "non_medical_content = [fetch_wikipedia_content(topic) for topic in non_medical_topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "medical_data = {\"text\": medical_content, \"label\": [\"medical\"] * len(medical_content)}\n",
        "non_medical_data = {\"text\": non_medical_content, \"label\": [\"non-medical\"] * len(non_medical_content)}\n",
        "\n",
        "df_medical = pd.DataFrame(medical_data)\n",
        "df_non_medical = pd.DataFrame(non_medical_data)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat([df_medical, df_non_medical], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjLzDMjwbX9s",
        "outputId": "9dad42c1-40ac-40ab-f61f-9bb9dc930b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                text        label\n",
            "0  Medicine is the science and practice of caring...      medical\n",
            "1  Cardiology from Ancient Greek kardi heart and ...      medical\n",
            "2  Surgery is a medical specialty that uses manua...      medical\n",
            "3  In common usage and medicine health according ...      medical\n",
            "4  Art is a diverse range of human activity and i...  non-medical\n",
            "5                                                     non-medical\n",
            "6  Literature is any collection of written work b...  non-medical\n",
            "7  Philosophy love of wisdom in ancient Greek is ...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = naive_bayes_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = naive_bayes_model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"Predictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "zVJs49NkcZ21",
        "outputId": "1c55883a-da2b-4179-cc7c-386afb336b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ceeab0853f44>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Model Selection and Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2155\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0;34m\"np.nan is an invalid document, expected byte or unicode string.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = naive_bayes_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = naive_bayes_model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"Predictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiCCXJVbcrEF",
        "outputId": "0d37917c-214e-45d3-f920-441b54e91061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.50      1.00      0.67         1\n",
            " non-medical       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "Predictions on new data:\n",
            "The human heart is a vital organ. - Predicted: non-medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = naive_bayes_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = naive_bayes_model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYgts_3MdCjx",
        "outputId": "2bf89a04-211f-4e3d-9cb0-9a80650f052c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        4\n",
            "non-medical    4\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.50      1.00      0.67         1\n",
            " non-medical       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "\n",
            "Predictions on new data:\n",
            "The human heart is a vital organ. - Predicted: non-medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = naive_bayes_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = naive_bayes_model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cross_val_scores = cross_val_score(naive_bayes_model, X_train_tfidf, y_train, cv=5)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"\\nCross-validation scores:\", cross_val_scores)\n",
        "print(\"Mean accuracy:\", cross_val_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "56Wp1xMRdfCu",
        "outputId": "27675e50-7360-4a3c-843c-d6ceef67f6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        4\n",
            "non-medical    4\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.50      1.00      0.67         1\n",
            " non-medical       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "\n",
            "Predictions on new data:\n",
            "The human heart is a vital organ. - Predicted: non-medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a4691029d7e6>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Perform 5-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mcross_val_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_bayes_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Print cross-validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0;31m# Sequentially call the tasks and yield the results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# pre_dispatch and n_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         iterable_with_config = (\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    350\u001b[0m             )\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0mmin_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0my_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0;34m\"n_splits=%d cannot be greater than the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0;34m\" number of members in each class.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = naive_bayes_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = naive_bayes_model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cross_val_scores = cross_val_score(naive_bayes_model, X_train_tfidf, y_train, cv=5)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"\\nCross-validation scores:\", cross_val_scores)\n",
        "print(\"Mean accuracy:\", cross_val_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "RsT2OjXqdnF3",
        "outputId": "f4ec07d3-5906-4bbb-d2c7-d09cb7d3e2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        4\n",
            "non-medical    4\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.50      1.00      0.67         1\n",
            " non-medical       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "\n",
            "Predictions on new data:\n",
            "The human heart is a vital organ. - Predicted: non-medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a4691029d7e6>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Perform 5-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mcross_val_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_bayes_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Print cross-validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0;31m# Sequentially call the tasks and yield the results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# pre_dispatch and n_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         iterable_with_config = (\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    350\u001b[0m             )\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0mmin_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0my_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0;34m\"n_splits=%d cannot be greater than the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0;34m\" number of members in each class.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "# Create a pipeline with SMOTE\n",
        "pipeline = Pipeline([('smote', SMOTE()), ('model', MultinomialNB())])\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cross_val_scores = cross_val_score(pipeline, X_train_tfidf, y_train, cv=5)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-validation scores:\", cross_val_scores)\n",
        "print(\"Mean accuracy:\", cross_val_scores.mean())\n",
        "\n",
        "# Fit the pipeline on the full training data\n",
        "pipeline.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = pipeline.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = pipeline.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "s5WQTkpEd6iw",
        "outputId": "8b3463bc-81f4-40a1-b626-ec1e09f3e107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        4\n",
            "non-medical    4\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-340d1f4d452a>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Perform 5-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mcross_val_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Print cross-validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m             \u001b[0;31m# Sequentially call the tasks and yield the results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# pre_dispatch and n_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         iterable_with_config = (\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    350\u001b[0m             )\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0mmin_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0my_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0;34m\"n_splits=%d cannot be greater than the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0;34m\" number of members in each class.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleaned_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "# Count the number of samples in the smallest class\n",
        "min_class_samples = min(y_train.value_counts())\n",
        "\n",
        "# Create a pipeline with SMOTE\n",
        "pipeline = Pipeline([('smote', SMOTE(sampling_strategy='auto', k_neighbors=min_class_samples-1)), ('model', MultinomialNB())])\n",
        "\n",
        "# Perform cross-validation with a number of splits less than or equal to the number of samples in the smallest class\n",
        "cross_val_splits = min(5, min_class_samples)\n",
        "cross_val_scores = cross_val_score(pipeline, X_train_tfidf, y_train, cv=cross_val_splits)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-validation scores:\", cross_val_scores)\n",
        "print(\"Mean accuracy:\", cross_val_scores.mean())\n",
        "\n",
        "# Fit the pipeline on the full training data\n",
        "pipeline.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = pipeline.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = pipeline.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-wzUtE3eG5P",
        "outputId": "ec248375-693d-47e8-d9b0-9ff7ac1d57b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        4\n",
            "non-medical    4\n",
            "Name: label, dtype: int64\n",
            "Cross-validation scores: [1. 1. 1.]\n",
            "Mean accuracy: 1.0\n",
            "\n",
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.50      1.00      0.67         1\n",
            " non-medical       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "\n",
            "Predictions on new data:\n",
            "The human heart is a vital organ. - Predicted: non-medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('cleaned_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "L-cJIpyvbzt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your dataset is loaded into df\n",
        "df = pd.read_csv(\"annotated_data.csv\")\n",
        "\n",
        "# Check the distribution of labels in the dataset\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Check the distribution of labels in the training and testing sets\n",
        "print(\"\\nTraining set labels distribution:\")\n",
        "print(train_labels.value_counts())\n",
        "\n",
        "print(\"\\nTesting set labels distribution:\")\n",
        "print(test_labels.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7xJ9_VoYTAD",
        "outputId": "2e7b5733-77d7-4983-da4c-121f9801a803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "medical        1\n",
            "non-medical    1\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Training set labels distribution:\n",
            "medical    1\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Testing set labels distribution:\n",
            "non-medical    1\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\", \"Historical_Figures\"\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": content_list, \"label\": [\"medical\"] * 12 + [\"non-medical\"] * 16}\n",
        "df_additional = pd.DataFrame(data)\n",
        "\n",
        "# Concatenate the new dataframe with the existing dataframe\n",
        "df = pd.concat([df, df_additional], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Qwg4D3U6gjPV",
        "outputId": "dd5b137b-9c9e-4743-e77b-1d405ccd7b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-83c6b87292fd>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Fetch content for all topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mcontent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfetch_wikipedia_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Create a DataFrame with the fetched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-83c6b87292fd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Fetch content for all topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mcontent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfetch_wikipedia_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Create a DataFrame with the fetched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-83c6b87292fd>\u001b[0m in \u001b[0;36mfetch_wikipedia_content\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Extract page ID and content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Clean the content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'extract'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract page ID and content\n",
        "    page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "    content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "    # Clean the content\n",
        "    cleaned_content = clean_text(content)\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\", \"Historical_Figures\"\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": content_list, \"label\": [\"medical\"] * 12 + [\"non-medical\"] * 16}\n",
        "df_additional = pd.DataFrame(data)\n",
        "\n",
        "# Concatenate the new dataframe with the existing dataframe\n",
        "df = pd.concat([df, df_additional], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Ex3uO4wNhuif",
        "outputId": "e40bdca9-ffad-4ee7-ce5c-25932939cc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3d75d5630713>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Fetch content for all topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mcontent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfetch_wikipedia_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Create a DataFrame with the fetched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-3d75d5630713>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Fetch content for all topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mcontent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfetch_wikipedia_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Create a DataFrame with the fetched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-3d75d5630713>\u001b[0m in \u001b[0;36mfetch_wikipedia_content\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Extract page ID and content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Clean the content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'extract'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the \"extract\" key is present in the response\n",
        "    if \"extract\" in data[\"query\"][\"pages\"][list(data[\"query\"][\"pages\"].keys())[0]]:\n",
        "        # Extract page ID and content\n",
        "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "        content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(content)\n",
        "\n",
        "        return cleaned_content\n",
        "    else:\n",
        "        # Print the title for which there is an issue\n",
        "        print(f\"Error fetching content for title: {title}\")\n",
        "        return None\n",
        "\n",
        "# Function\n"
      ],
      "metadata": {
        "id": "B_ot5ALlh7Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the \"extract\" key is present in the response\n",
        "    if \"extract\" in data[\"query\"][\"pages\"][list(data[\"query\"][\"pages\"].keys())[0]]:\n",
        "        # Extract page ID and content\n",
        "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "        content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(content)\n",
        "\n",
        "        return cleaned_content\n",
        "    else:\n",
        "        # Print the title for which there is an issue\n",
        "        print(f\"Error fetching content for title: {title}\")\n",
        "        return None\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\", \"Historical_Figures\"\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Filter out None values (titles with issues)\n",
        "content_list = [content for content in content_list if content is not None]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": content_list, \"label\": [\"medical\"] * 12 + [\"non-medical\"] * 16}\n",
        "df_additional = pd.DataFrame(data)\n",
        "\n",
        "# Concatenate the new dataframe with the existing dataframe\n",
        "df = pd.concat([df, df_additional], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "I4GOyeJ5iI02",
        "outputId": "d4d8c1cd-ab8f-44ef-891f-d928c6bc4431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching content for title: Historical_Figures\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-fb8e537c53e2>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Create a DataFrame with the fetched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontent_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"medical\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"non-medical\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mdf_additional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Concatenate the new dataframe with the existing dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the \"extract\" key is present in the response\n",
        "    if \"extract\" in data[\"query\"][\"pages\"][list(data[\"query\"][\"pages\"].keys())[0]]:\n",
        "        # Extract page ID and content\n",
        "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "        content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(content)\n",
        "\n",
        "        return cleaned_content\n",
        "    else:\n",
        "        # Print the title for which there is an issue\n",
        "        print(f\"Error fetching content for title: {title}\")\n",
        "        return None\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\", \"Historical_Figures\"\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Filter out None values (titles with issues)\n",
        "filtered_content_list = [content for content in content_list if content is not None]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": filtered_content_list, \"label\": [\"medical\"] * 12 + [\"non-medical\"] * 16}\n",
        "df_additional = pd.DataFrame(data)\n",
        "\n",
        "# Concatenate the new dataframe with the existing dataframe\n",
        "df = pd.concat([df, df_additional], ignore_index=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset:\")\n",
        "print(df)\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "NoLhCxj1iXPf",
        "outputId": "4816f93e-94f7-4e3e-bbf2-c676de2ddc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching content for title: Historical_Figures\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-33ba5fc53c55>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Create a DataFrame with the fetched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfiltered_content_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"medical\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"non-medical\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mdf_additional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Concatenate the new dataframe with the existing dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the \"extract\" key is present in the response\n",
        "    if \"extract\" in data[\"query\"][\"pages\"][list(data[\"query\"][\"pages\"].keys())[0]]:\n",
        "        # Extract page ID and content\n",
        "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "        content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(content)\n",
        "\n",
        "        return cleaned_content\n",
        "    else:\n",
        "        # Print the title for which there is an issue\n",
        "        print(f\"Error fetching content for title: {title}\")\n",
        "        return None\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\", \"Historical_Figures\"\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Filter out None values (titles with issues)\n",
        "filtered_content_list = [content for content in content_list if content is not None]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": filtered_content_list, \"label\": [\"medical\"] * 12 + [\"non-medical\"] * 16}\n",
        "\n",
        "try:\n",
        "    df_additional = pd.DataFrame(data)\n",
        "    # Concatenate the new dataframe with the existing dataframe\n",
        "    df = pd.concat([df, df_additional], ignore_index=True)\n",
        "    # Check the updated dataset\n",
        "    print(\"Updated Dataset:\")\n",
        "    print(df)\n",
        "    df.to_csv('cleaned_dataset.csv', index=False)\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT1DsHUCings",
        "outputId": "0ae84f1d-300d-40bd-acae-d27248e349ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching content for title: Historical_Figures\n",
            "ValueError: All arrays must be of the same length\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the \"extract\" key is present in the response\n",
        "    if \"extract\" in data[\"query\"][\"pages\"][list(data[\"query\"][\"pages\"].keys())[0]]:\n",
        "        # Extract page ID and content\n",
        "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "        content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(content)\n",
        "\n",
        "        return cleaned_content\n",
        "    else:\n",
        "        # Print the title for which there is an issue\n",
        "        print(f\"Error fetching content for title: {title}\")\n",
        "        return None\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\"\n",
        "    # Removed \"Historical_Figures\" to avoid the issue\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Filter out None values (titles with issues)\n",
        "filtered_content_list = [content for content in content_list if content is not None]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": filtered_content_list, \"label\": [\"medical\"] * 11 + [\"non-medical\"] * 16}\n",
        "\n",
        "try:\n",
        "    df_additional = pd.DataFrame(data)\n",
        "    # Concatenate the new dataframe with the existing dataframe\n",
        "    df = pd.concat([df, df_additional], ignore_index=True)\n",
        "    # Check the updated dataset\n",
        "    print(\"Updated Dataset:\")\n",
        "    print(df)\n",
        "    df.to_csv('cleaned_dataset.csv', index=False)\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTiwOA2ji4o3",
        "outputId": "b3f17025-1314-445f-b61f-0b338fb15f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Medicine is the science and practice of caring...      medical\n",
            "1   Cardiology from Ancient Greek kardi heart and ...      medical\n",
            "2   Surgery is a medical specialty that uses manua...      medical\n",
            "3   In common usage and medicine health according ...      medical\n",
            "4                                                          medical\n",
            "5   Pharmacy is the science and practice of discov...      medical\n",
            "6   Immunology is a branch of biology and medicine...      medical\n",
            "7   Pathology is the study of disease and injury T...      medical\n",
            "8   Pediatrics also spelled paediatrics or pdiatri...      medical\n",
            "9   Oncology is a branch of medicine that deals wi...      medical\n",
            "10  Neurology from Greek neron string nerve and th...      medical\n",
            "11  Dentistry also known as dental medicine and or...  non-medical\n",
            "12  Art is a diverse range of human activity and i...  non-medical\n",
            "13                                                     non-medical\n",
            "14  Literature is any collection of written work b...  non-medical\n",
            "15  Philosophy love of wisdom in ancient Greek is ...  non-medical\n",
            "16  Science is a rigorous systematic endeavor that...  non-medical\n",
            "17  Technology is the application of conceptual kn...  non-medical\n",
            "18  Space is a threedimensional continuum containi...  non-medical\n",
            "19  Environment most often refers to Natural envir...  non-medical\n",
            "20  Food is any substance consumed by an organism ...  non-medical\n",
            "21  A cuisine is a style of cooking characterized ...  non-medical\n",
            "22                                                     non-medical\n",
            "23  Cooking also known as cookery or professionall...  non-medical\n",
            "24  History derived from Ancient Greek histora inq...  non-medical\n",
            "25                                                     non-medical\n",
            "26  Archaeology or archeology is the study of huma...  non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to fetch content from Wikipedia\n",
        "def fetch_wikipedia_content(title):\n",
        "    api_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "    }\n",
        "    response = requests.get(api_endpoint, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if the \"extract\" key is present in the response\n",
        "    if \"extract\" in data[\"query\"][\"pages\"][list(data[\"query\"][\"pages\"].keys())[0]]:\n",
        "        # Extract page ID and content\n",
        "        page_id = list(data[\"query\"][\"pages\"].keys())[0]\n",
        "        content = data[\"query\"][\"pages\"][page_id][\"extract\"]\n",
        "\n",
        "        # Clean the content\n",
        "        cleaned_content = clean_text(content)\n",
        "\n",
        "        return cleaned_content\n",
        "    else:\n",
        "        # Print the title for which there is an issue\n",
        "        print(f\"Error fetching content for title: {title}\")\n",
        "        return None\n",
        "\n",
        "# Function to clean text (remove HTML tags, references, etc.)\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and comments\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    clean_text = soup.get_text(separator=\" \")\n",
        "\n",
        "    # Remove special characters and non-alphabetic characters\n",
        "    clean_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", clean_text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Create an initial DataFrame\n",
        "df = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "\n",
        "# List of topics to fetch\n",
        "topics = [\n",
        "    \"Medicine\", \"Cardiology\", \"Surgery\", \"Health\",\"Medical_Imaging\", \"Pharmacy\", \"Immunology\", \"Pathology\",\n",
        "    \"Pediatrics\", \"Oncology\", \"Neurology\", \"Dentistry\",\n",
        "    \"Art\", \"History_of_Art\", \"Literature\", \"Philosophy\",\n",
        "    \"Science\", \"Technology\", \"Space\", \"Environment\",\n",
        "    \"Food\", \"Cuisine\", \"Recipes\", \"Cooking\",\n",
        "    \"History\", \"Ancient_Civilizations\", \"Archaeology\"\n",
        "    # Removed \"Historical_Figures\" to avoid the issue\n",
        "]\n",
        "\n",
        "# Fetch content for all topics\n",
        "content_list = [fetch_wikipedia_content(topic) for topic in topics]\n",
        "\n",
        "# Filter out None values (titles with issues)\n",
        "filtered_content_list = [content for content in content_list if content is not None]\n",
        "\n",
        "# Create a DataFrame with the fetched data\n",
        "data = {\"text\": filtered_content_list, \"label\": [\"medical\"] * 11 + [\"non-medical\"] * 16}\n",
        "\n",
        "try:\n",
        "    df_additional = pd.DataFrame(data)\n",
        "    # Concatenate the new dataframe with the existing dataframe\n",
        "    df = pd.concat([df, df_additional], ignore_index=True)\n",
        "    # Check the updated dataset\n",
        "    print(\"Updated Dataset:\")\n",
        "    print(df)\n",
        "    df.to_csv('cleanedd_dataset.csv', index=False)\n",
        "    print(\"Dataset saved as 'cleaned_dataset.csv'\")\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Iw0-myQjihO",
        "outputId": "b1db5a4c-b066-40a8-884c-08846d5f461e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset:\n",
            "                                                 text        label\n",
            "0   Medicine is the science and practice of caring...      medical\n",
            "1   Cardiology from Ancient Greek kardi heart and ...      medical\n",
            "2   Surgery is a medical specialty that uses manua...      medical\n",
            "3   In common usage and medicine health according ...      medical\n",
            "4                                                          medical\n",
            "5   Pharmacy is the science and practice of discov...      medical\n",
            "6   Immunology is a branch of biology and medicine...      medical\n",
            "7   Pathology is the study of disease and injury T...      medical\n",
            "8   Pediatrics also spelled paediatrics or pdiatri...      medical\n",
            "9   Oncology is a branch of medicine that deals wi...      medical\n",
            "10  Neurology from Greek neron string nerve and th...      medical\n",
            "11  Dentistry also known as dental medicine and or...  non-medical\n",
            "12  Art is a diverse range of human activity and i...  non-medical\n",
            "13                                                     non-medical\n",
            "14  Literature is any collection of written work b...  non-medical\n",
            "15  Philosophy love of wisdom in ancient Greek is ...  non-medical\n",
            "16  Science is a rigorous systematic endeavor that...  non-medical\n",
            "17  Technology is the application of conceptual kn...  non-medical\n",
            "18  Space is a threedimensional continuum containi...  non-medical\n",
            "19  Environment most often refers to Natural envir...  non-medical\n",
            "20  Food is any substance consumed by an organism ...  non-medical\n",
            "21  A cuisine is a style of cooking characterized ...  non-medical\n",
            "22                                                     non-medical\n",
            "23  Cooking also known as cookery or professionall...  non-medical\n",
            "24  History derived from Ancient Greek histora inq...  non-medical\n",
            "25                                                     non-medical\n",
            "26  Archaeology or archeology is the study of huma...  non-medical\n",
            "Dataset saved as 'cleaned_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleanedd_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Handle missing values in X_train\n",
        "X_train = X_train.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Selection and Training\n",
        "pipeline = Pipeline([('model', MultinomialNB(class_prior=None))])\n",
        "\n",
        "# Perform cross-validation with StratifiedKFold\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cross_val_results = cross_validate(pipeline, X_train_tfidf, y_train, cv=cv, scoring=['accuracy'], return_train_score=False)\n",
        "cross_val_scores = cross_val_results['test_accuracy']\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Cross-validation scores:\", cross_val_scores)\n",
        "print(\"Mean accuracy:\", cross_val_scores.mean())\n",
        "\n",
        "# Fit the pipeline on the full training data\n",
        "pipeline.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = pipeline.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"The human heart is a vital organ.\", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = pipeline.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVrygO2Pmz8R",
        "outputId": "b415205c-95b2-4641-df46-2b0351d7e7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "non-medical    16\n",
            "medical        11\n",
            "Name: label, dtype: int64\n",
            "Cross-validation scores: [0.6  0.75 0.75 0.5  0.5 ]\n",
            "Mean accuracy: 0.62\n",
            "\n",
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.00      0.00      0.00         3\n",
            " non-medical       0.50      1.00      0.67         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.25      0.50      0.33         6\n",
            "weighted avg       0.25      0.50      0.33         6\n",
            "\n",
            "\n",
            "Predictions on new data:\n",
            "The human heart is a vital organ. - Predicted: non-medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_csv('cleanedd_dataset.csv')\n",
        "\n",
        "# Check class distribution\n",
        "print(\"Original Data Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Data Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in X_test\n",
        "X_test = X_test.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Handle missing values in X_train\n",
        "X_train = X_train.fillna('')  # Replace NaN with an empty string or any other placeholder\n",
        "\n",
        "# Feature Extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Resampling using SMOTE and RandomUnderSampler\n",
        "sampler = SMOTE(sampling_strategy='auto', k_neighbors=5)\n",
        "X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Selection and Training\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Model Evaluation\n",
        "predictions = model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "\n",
        "# Example of making predictions on new data\n",
        "new_data = [\"heart disease is hard \", \"Artistic expression varies across cultures.\"]\n",
        "new_data_tfidf = tfidf_vectorizer.transform(new_data)\n",
        "new_predictions = model.predict(new_data_tfidf)\n",
        "\n",
        "print(\"\\nPredictions on new data:\")\n",
        "for text, prediction in zip(new_data, new_predictions):\n",
        "    print(f\"{text} - Predicted: {prediction}\")\n"
      ],
      "metadata": {
        "id": "dChPOxSJn9jG",
        "outputId": "9f00c6da-6588-4199-d1d0-18633f38dc8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Distribution:\n",
            "non-medical    16\n",
            "medical        11\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Accuracy: 0.6666666666666666\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     medical       0.60      1.00      0.75         3\n",
            " non-medical       1.00      0.33      0.50         3\n",
            "\n",
            "    accuracy                           0.67         6\n",
            "   macro avg       0.80      0.67      0.62         6\n",
            "weighted avg       0.80      0.67      0.62         6\n",
            "\n",
            "\n",
            "Predictions on new data:\n",
            "heart disease is hard  - Predicted: medical\n",
            "Artistic expression varies across cultures. - Predicted: non-medical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YeGgXH0djPAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}